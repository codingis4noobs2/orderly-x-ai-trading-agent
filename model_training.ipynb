{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud==0.34.0 google-cloud-bigquery matplotlib pandas_ta scikit-learn emp-orderly-types emp-orderly setuptools ccxt pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import joblib\n",
    "import ccxt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Suppress any warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized GCP clients for project: streamlit-apps-431010\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve GCP project ID and credentials path from environment variables\n",
    "project_id = os.getenv(\"GCP_PROJECT_ID\")\n",
    "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "# Ensure that both environment variables are available\n",
    "if not project_id or not credentials_path:\n",
    "    raise ValueError(\"GCP_PROJECT_ID and GOOGLE_APPLICATION_CREDENTIALS must be set in the .env file.\")\n",
    "\n",
    "# Initialize Google Cloud credentials and clients\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "bigquery_client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "storage_client = storage.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "print(f\"Successfully initialized GCP clients for project: {project_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_ohlcv_data(exchange_name, symbol, timeframe, since):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data from a specified exchange using the ccxt library.\n",
    "\n",
    "    Parameters:\n",
    "    - exchange_name (str): The name of the exchange (e.g., 'binance').\n",
    "    - symbol (str): The trading pair symbol (e.g., 'BTC/USDT').\n",
    "    - timeframe (str): The timeframe for the OHLCV data (e.g., '1h', '1d').\n",
    "    - since (int): Timestamp (in milliseconds) from which to start fetching data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing OHLCV data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exchange = getattr(ccxt, exchange_name)()\n",
    "        exchange.load_markets()\n",
    "\n",
    "        all_data = []\n",
    "        while since < time.time() * 1000:\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=1000)\n",
    "            if len(ohlcv) == 0:\n",
    "                break\n",
    "            all_data.extend(ohlcv)\n",
    "            since = ohlcv[-1][0] + 1\n",
    "            time.sleep(exchange.rateLimit / 1000)\n",
    "\n",
    "        df = pd.DataFrame(all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_bigquery(dataframe, table_id):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to a Google BigQuery table.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The DataFrame to be saved.\n",
    "    - table_id (str): The BigQuery table identifier in the format 'dataset.table'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe.to_gbq(destination_table=table_id, project_id=project_id, if_exists='replace', credentials=credentials)\n",
    "        print(f\"Data successfully saved to BigQuery table: {table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_gcs(model, scaler, bucket_name, model_filename, scaler_filename):\n",
    "    \"\"\"\n",
    "    Save a machine learning model and its corresponding scaler to a Google Cloud Storage bucket.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained machine learning model.\n",
    "    - scaler: The scaler object used for data preprocessing.\n",
    "    - bucket_name (str): GCS bucket name.\n",
    "    - model_filename (str): Filename for the model.\n",
    "    - scaler_filename (str): Filename for the scaler.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        if not bucket.exists():\n",
    "            bucket = storage_client.create_bucket(bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' created.\")\n",
    "\n",
    "        joblib.dump(model, model_filename)\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "        model_blob = bucket.blob(model_filename)\n",
    "        scaler_blob = bucket.blob(scaler_filename)\n",
    "\n",
    "        model_blob.upload_from_filename(model_filename)\n",
    "        scaler_blob.upload_from_filename(scaler_filename)\n",
    "\n",
    "        print(f\"Model and scaler saved to GCS bucket: {bucket_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to GCS: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Add common technical indicators to a given price DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing OHLCV data.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with added technical indicators.\n",
    "    \"\"\"\n",
    "    df['SMA_10'] = ta.sma(df['close'], length=10)\n",
    "    df['SMA_20'] = ta.sma(df['close'], length=20)\n",
    "    df['RSI'] = ta.rsi(df['close'], length=14)\n",
    "\n",
    "    macd = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
    "    df['MACD'] = macd['MACD_12_26_9']\n",
    "    df['MACD_signal'] = macd['MACDs_12_26_9']\n",
    "\n",
    "    bbands = ta.bbands(df['close'], length=20)\n",
    "    df['BB_upper'] = bbands['BBU_20_2.0']\n",
    "    df['BB_middle'] = bbands['BBM_20_2.0']\n",
    "    df['BB_lower'] = bbands['BBL_20_2.0']\n",
    "\n",
    "    df['ATR'] = ta.atr(df['high'], df['low'], df['close'], length=14)\n",
    "    df['MOM'] = ta.mom(df['close'], length=10)\n",
    "    df['ROC'] = ta.roc(df['close'], length=10)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(df):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix and target vector for training a machine learning model.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with technical indicators and price data.\n",
    "    \n",
    "    Returns:\n",
    "    - Scaled training and testing data along with the target vectors.\n",
    "    \"\"\"\n",
    "    df['target'] = np.where(df['close'].shift(-1) > df['close'] * 1.0035, 1, 0)\n",
    "    \n",
    "    features = ['SMA_10', 'SMA_20', 'RSI', 'MACD', 'MACD_signal', 'BB_upper', 'BB_middle', 'BB_lower', 'ATR', 'MOM', 'ROC']\n",
    "    X = df[features]\n",
    "    y = df['target']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to BigQuery table: crypto_dataset.raw_prices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to BigQuery table: crypto_dataset.processed_prices\n",
      "Model Accuracy: 0.9480563389573334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97    115144\n",
      "           1       0.31      0.02      0.04      6122\n",
      "\n",
      "    accuracy                           0.95    121266\n",
      "   macro avg       0.63      0.51      0.51    121266\n",
      "weighted avg       0.92      0.95      0.93    121266\n",
      "\n",
      "Model and scaler successfully saved to GCS bucket: 'streamlit-apps-431010-crypto_trading_bucket'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    exchange_name = \"kucoin\"\n",
    "    symbol = \"ETH/USDT\"\n",
    "    timeframe = \"5m\"\n",
    "    since = int(time.mktime(time.strptime('2019-01-01', '%Y-%m-%d'))) * 1000\n",
    "\n",
    "    data = fetch_ohlcv_data(exchange_name, symbol, timeframe, since)\n",
    "\n",
    "    if data is not None:\n",
    "        save_to_bigquery(data, \"crypto_dataset.raw_prices\")\n",
    "        data = add_technical_indicators(data)\n",
    "        save_to_bigquery(data, \"crypto_dataset.processed_prices\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test, scaler = prepare_training_data(data)\n",
    "\n",
    "        model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"Model Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        save_model_to_gcs(\n",
    "            model, \n",
    "            scaler, \n",
    "            bucket_name=f'{project_id}-crypto_trading_bucket', \n",
    "            model_filename='trading_model.pkl', \n",
    "            scaler_filename='scaler.pkl'\n",
    "        )\n",
    "    else:\n",
    "        print(\"Data fetching failed. Please check your parameters and try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
