{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q google-cloud==0.34.0 google-cloud-bigquery matplotlib pandas_ta scikit-learn emp-orderly-types emp-orderly setuptools ccxt pandas-gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "\n",
    "# Standard library imports\n",
    "import asyncio  # For asynchronous operations\n",
    "import time  # For time-based operations and timestamps\n",
    "import os  # For environment management and file operations\n",
    "import warnings  # To filter out warnings\n",
    "\n",
    "# Importing to fetch environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Third-party library imports\n",
    "import matplotlib.pyplot as plt  # For plotting and visualization\n",
    "import numpy as np  # For numerical operations and array manipulations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import pandas_ta as ta  # For technical analysis indicators and tools\n",
    "import joblib  # For model serialization and deserialization\n",
    "import ccxt  # For cryptocurrency trading APIs and market data retrieval\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # For ML model training\n",
    "from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets\n",
    "from sklearn.preprocessing import StandardScaler  # For feature scaling\n",
    "from sklearn.metrics import accuracy_score, classification_report  # For model evaluation\n",
    "\n",
    "# Google Cloud imports\n",
    "from google.cloud import bigquery  # For interacting with Google BigQuery\n",
    "from google.cloud import storage  # For interacting with Google Cloud Storage\n",
    "from google.oauth2 import service_account  # For Google Cloud authentication\n",
    "\n",
    "# Empyreal SDK imports for strategy development and backtesting\n",
    "from emp_orderly import Strategy, EmpOrderly  # For strategy implementation and management\n",
    "from emp_orderly_types import PerpetualAssetType, Interval  # For defining asset types and intervals\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized GCP clients for project: streamlit-apps-431010\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Retrieve GCP project ID and credentials path from environment variables\n",
    "project_id = os.getenv(\"GCP_PROJECT_ID\")\n",
    "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
    "\n",
    "# Ensure that both environment variables are available\n",
    "if not project_id or not credentials_path:\n",
    "    raise ValueError(\"GCP_PROJECT_ID and GOOGLE_APPLICATION_CREDENTIALS must be set in the .env file.\")\n",
    "\n",
    "# Initialize Google Cloud service account credentials using the specified JSON file\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_path)\n",
    "\n",
    "# Create a BigQuery client using the specified credentials and project ID\n",
    "client = bigquery.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "# Create a Storage client using the same credentials and project ID\n",
    "storage_client = storage.Client(credentials=credentials, project=project_id)\n",
    "\n",
    "print(f\"Successfully initialized GCP clients for project: {project_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch OHLCV (Open, High, Low, Close, Volume) data from a specified exchange\n",
    "def fetch_ohlcv_data(exchange_name, symbol, timeframe, since):\n",
    "    \"\"\"\n",
    "    Fetch OHLCV data from a specified exchange using the ccxt library.\n",
    "    \n",
    "    Parameters:\n",
    "    - exchange_name (str): The name of the exchange (e.g., 'binance').\n",
    "    - symbol (str): The trading pair symbol (e.g., 'BTC/USDT').\n",
    "    - timeframe (str): The timeframe for the OHLCV data (e.g., '1h', '1d').\n",
    "    - since (int): Timestamp (in milliseconds) from which to start fetching data.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing OHLCV data with columns ['timestamp', 'open', 'high', 'low', 'close', 'volume'].\n",
    "    - None: If an error occurs while fetching data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize the exchange instance using the exchange name provided\n",
    "        exchange = getattr(ccxt, exchange_name)()\n",
    "        \n",
    "        # Load exchange markets to ensure the symbol is recognized\n",
    "        exchange.load_markets()\n",
    "\n",
    "        # List to store all OHLCV data\n",
    "        all_data = []\n",
    "\n",
    "        # Fetch data iteratively until the current time\n",
    "        while since < time.time() * 1000:\n",
    "            # Fetch OHLCV data with a limit of 1000 records to avoid hitting rate limits\n",
    "            ohlcv = exchange.fetch_ohlcv(symbol, timeframe=timeframe, since=since, limit=1000)\n",
    "            \n",
    "            # Break if no data is returned\n",
    "            if len(ohlcv) == 0:\n",
    "                break\n",
    "            \n",
    "            # Extend the list with the fetched data\n",
    "            all_data.extend(ohlcv)\n",
    "\n",
    "            # Update 'since' to the last fetched timestamp + 1ms to avoid overlap\n",
    "            since = ohlcv[-1][0] + 1\n",
    "            \n",
    "            # Respect the exchange rate limit to avoid getting banned ;)\n",
    "            time.sleep(exchange.rateLimit / 1000)\n",
    "\n",
    "        # Convert the collected data to a pandas DataFrame\n",
    "        df = pd.DataFrame(\n",
    "            all_data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "        )\n",
    "        \n",
    "        # Convert the 'timestamp' from milliseconds to a datetime format\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "# User-defined parameters for fetching OHLCV data\n",
    "exchange_name = \"kucoin\"\n",
    "symbol = \"BTC/USDT\"\n",
    "timeframe = \"30m\"\n",
    "\n",
    "# Define the starting date for fetching historical data and convert it to timestamp (ms)\n",
    "since = int(time.mktime(time.strptime('2020-01-01', '%Y-%m-%d'))) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a DataFrame to a specified Google BigQuery table\n",
    "def save_to_bigquery(dataframe, table_id):\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to a Google BigQuery table.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe (pd.DataFrame): The DataFrame to be saved.\n",
    "    - table_id (str): The BigQuery table identifier in the format 'dataset.table'.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Prints a success message if the operation is successful, \n",
    "            otherwise prints an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Save the DataFrame to the specified BigQuery table\n",
    "        dataframe.to_gbq(\n",
    "            destination_table=table_id,\n",
    "            project_id=project_id,\n",
    "            if_exists='replace',\n",
    "            credentials=credentials\n",
    "        )\n",
    "        \n",
    "        print(f\"Data successfully saved to BigQuery table: {table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to BigQuery: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save a trained model and its scaler to Google Cloud Storage (GCS)\n",
    "def save_model_to_gcs(model, scaler, bucket_name, model_filename, scaler_filename):\n",
    "    \"\"\"\n",
    "    Save a machine learning model and its corresponding scaler to a Google Cloud Storage bucket.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The machine learning model object to be saved.\n",
    "    - scaler: The scaler object used for data preprocessing.\n",
    "    - bucket_name (str): Name of the GCS bucket where the model and scaler will be saved.\n",
    "    - model_filename (str): Filename to use for saving the model.\n",
    "    - scaler_filename (str): Filename to use for saving the scaler.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Prints success or error messages based on the outcome of the operation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get or create the GCS bucket\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        \n",
    "        # Check if the bucket exists; if not, create it\n",
    "        if not bucket.exists():\n",
    "            bucket = storage_client.create_bucket(bucket_name)\n",
    "            print(f\"Bucket '{bucket_name}' created.\")\n",
    "\n",
    "        # Create blobs (object placeholders) in the bucket for the model and scaler files\n",
    "        model_blob = bucket.blob(model_filename)\n",
    "        scaler_blob = bucket.blob(scaler_filename)\n",
    "\n",
    "        # Save the model and scaler to local files\n",
    "        joblib.dump(model, model_filename)\n",
    "        joblib.dump(scaler, scaler_filename)\n",
    "\n",
    "        # Upload the local model and scaler files to the corresponding GCS blobs\n",
    "        model_blob.upload_from_filename(model_filename)\n",
    "        scaler_blob.upload_from_filename(scaler_filename)\n",
    "\n",
    "        print(f\"Model and scaler successfully saved to GCS bucket: '{bucket_name}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to GCS: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add technical indicators to a given DataFrame\n",
    "def add_technical_indicators(df):\n",
    "    \"\"\"\n",
    "    Add common technical indicators to a given price DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing OHLCV data with columns ['open', 'high', 'low', 'close', 'volume'].\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The original DataFrame with additional columns for each technical indicator.\n",
    "    \"\"\"\n",
    "    # Add Simple Moving Averages (SMA) for 10 and 20 periods\n",
    "    df['SMA_10'] = ta.sma(df['close'], length=10)\n",
    "    df['SMA_20'] = ta.sma(df['close'], length=20)\n",
    "\n",
    "    # Add Relative Strength Index (RSI) with a length of 14 periods\n",
    "    df['RSI'] = ta.rsi(df['close'], length=14)\n",
    "\n",
    "    # Add Moving Average Convergence Divergence (MACD) with fast=12, slow=26, signal=9\n",
    "    macd = ta.macd(df['close'], fast=12, slow=26, signal=9)\n",
    "    df['MACD'] = macd['MACD_12_26_9']\n",
    "    df['MACD_signal'] = macd['MACDs_12_26_9']\n",
    "\n",
    "    # Add Bollinger Bands (BB) with length of 20 periods and a standard deviation of 2.0\n",
    "    bbands = ta.bbands(df['close'], length=20)\n",
    "    df['BB_upper'] = bbands['BBU_20_2.0']\n",
    "    df['BB_middle'] = bbands['BBM_20_2.0']\n",
    "    df['BB_lower'] = bbands['BBL_20_2.0']\n",
    "\n",
    "    # Add Average True Range (ATR) with a length of 14 periods\n",
    "    df['ATR'] = ta.atr(df['high'], df['low'], df['close'], length=14)\n",
    "\n",
    "    # Add Momentum (MOM) indicator with a length of 10 periods\n",
    "    df['MOM'] = ta.mom(df['close'], length=10)\n",
    "\n",
    "    # Add Rate of Change (ROC) indicator with a length of 10 periods\n",
    "    df['ROC'] = ta.roc(df['close'], length=10)\n",
    "\n",
    "    # Drop rows with NaN values that may result from indicator calculations\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare training data for machine learning models\n",
    "def prepare_training_data(df):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix and target vector for training a machine learning model.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing technical indicators and price data.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train (np.array): Scaled training feature matrix.\n",
    "    - X_test (np.array): Scaled testing feature matrix.\n",
    "    - y_train (np.array): Training target vector.\n",
    "    - y_test (np.array): Testing target vector.\n",
    "    - scaler (StandardScaler): Fitted scaler object for future scaling transformations.\n",
    "    \"\"\"\n",
    "    # Define target variable: 1 if next close price is 0.5% higher, otherwise 0\n",
    "    df['target'] = np.where(df['close'].shift(-1) > df['close'] * 1.005, 1, 0)\n",
    "\n",
    "    # Define the feature set used for model training\n",
    "    features = [\n",
    "        'SMA_10', 'SMA_20', 'RSI', 'MACD', 'MACD_signal',\n",
    "        'BB_upper', 'BB_middle', 'BB_lower', 'ATR', 'MOM', 'ROC'\n",
    "    ]\n",
    "\n",
    "    # Create feature matrix X and target vector y\n",
    "    X = df[features]\n",
    "    y = df['target']\n",
    "\n",
    "    # Split the data into training and testing sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize a StandardScaler to normalize the feature values\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on the training data and transform both training and testing data\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Return the processed training and testing data along with the fitted scaler\n",
    "    return X_train, X_test, y_train, y_test, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to BigQuery table: crypto_dataset.raw_prices\n",
      "Data successfully saved to BigQuery table: crypto_dataset.processed_prices\n",
      "Model Accuracy: 0.9162128194386259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96     15358\n",
      "           1       0.31      0.03      0.05      1351\n",
      "\n",
      "    accuracy                           0.92     16709\n",
      "   macro avg       0.61      0.51      0.50     16709\n",
      "weighted avg       0.87      0.92      0.88     16709\n",
      "\n",
      "Model and scaler successfully saved to GCS bucket: 'streamlit-apps-431010-crypto_trading_bucket'\n"
     ]
    }
   ],
   "source": [
    "# Main function for fetching data, feature engineering, training, and saving the model\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Main function for executing the end-to-end process of data fetching, feature engineering,\n",
    "    training the model, evaluating the model, and saving the model and scaler to Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    # Fetch the historical data using the defined parameters\n",
    "    data = fetch_ohlcv_data(exchange_name, symbol, timeframe, since)\n",
    "\n",
    "    # Check if data was successfully fetched\n",
    "    if data is not None:\n",
    "        # Save the raw OHLCV data to Google BigQuery\n",
    "        save_to_bigquery(data, \"crypto_dataset.raw_prices\")\n",
    "\n",
    "        # Add technical indicators to the fetched data\n",
    "        data = add_technical_indicators(data)\n",
    "\n",
    "        # Save the processed data (with technical indicators) to Google BigQuery\n",
    "        save_to_bigquery(data, \"crypto_dataset.processed_prices\")\n",
    "\n",
    "        # Prepare the data for model training by splitting into train/test sets and scaling\n",
    "        X_train, X_test, y_train, y_test, scaler = prepare_training_data(data)\n",
    "\n",
    "        # Initialize and train a RandomForestClassifier\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate the trained model using the testing data\n",
    "        y_pred = model.predict(X_test)\n",
    "        print(f\"Model Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # Save the trained model and scaler to Google Cloud Storage\n",
    "        save_model_to_gcs(\n",
    "            model, \n",
    "            scaler, \n",
    "            bucket_name=f'{project_id}-crypto_trading_bucket', \n",
    "            model_filename='trading_model.pkl', \n",
    "            scaler_filename='scaler.pkl'\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Data fetching failed. Please check your parameters and try again.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
